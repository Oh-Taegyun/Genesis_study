``` python
import torch
import math
import genesis as gs
from genesis.utils.geom import quat_to_xyz, transform_by_quat, inv_quat, transform_quat_by_quat

# 지정된 범위 내에서 랜덤한 float 값을 생성하는 함수
def gs_rand_float(lower, upper, shape, device):
    return (upper - lower) * torch.rand(size=shape, device=device) + lower


# HoverEnv 클래스 정의
class HoverEnv:
    def __init__(self, num_envs, env_cfg, obs_cfg, reward_cfg, command_cfg, show_viewer=False, device="cuda"):
        # 디바이스 설정 (기본값: CUDA)
        self.device = torch.device(device)

        # 환경 변수 설정
        self.num_envs = num_envs  # 병렬로 실행될 환경 개수
        self.num_obs = obs_cfg["num_obs"]  # 관측 벡터 크기
        self.num_privileged_obs = None  # 특권 관측 변수 (현재 사용 안 함)
        self.num_actions = env_cfg["num_actions"]  # 행동 벡터 크기
        self.num_commands = command_cfg["num_commands"]  # 명령 벡터 크기

        # 행동 지연 시뮬레이션 여부
        self.simulate_action_latency = env_cfg["simulate_action_latency"]
        self.dt = 0.01  # 100Hz로 시뮬레이션 실행
        self.max_episode_length = math.ceil(env_cfg["episode_length_s"] / self.dt)  # 최대 에피소드 길이 계산

        # 환경, 관측, 보상, 명령 설정 저장
        self.env_cfg = env_cfg
        self.obs_cfg = obs_cfg
        self.reward_cfg = reward_cfg
        self.command_cfg = command_cfg

        # 관측 및 보상 스케일 설정
        self.obs_scales = obs_cfg["obs_scales"]
        self.reward_scales = reward_cfg["reward_scales"]

        # 시뮬레이션 장면 생성
        self.scene = gs.Scene(
            sim_options=gs.options.SimOptions(dt=self.dt, substeps=2),  # 시뮬레이션 시간 설정
            viewer_options=gs.options.ViewerOptions(  # 뷰어 옵션 설정
                max_FPS=env_cfg["max_visualize_FPS"],
                camera_pos=(3.0, 0.0, 3.0),
                camera_lookat=(0.0, 0.0, 1.0),
                camera_fov=40,
            ),
            vis_options=gs.options.VisOptions(n_rendered_envs=10),  # 렌더링할 환경 수
            rigid_options=gs.options.RigidOptions(  # 강체 시뮬레이션 설정
                dt=self.dt,
                constraint_solver=gs.constraint_solver.Newton,  # 뉴턴 방식 제약 조건 해결기
                enable_collision=True,  # 충돌 감지 활성화
                enable_joint_limit=True,  # 관절 제한 활성화
            ),
            show_viewer=show_viewer,  # 뷰어 표시 여부
        )

        # 지면(Plane) 추가
        self.scene.add_entity(gs.morphs.Plane())

        # 목표 지점 추가 (시각화 활성화 여부에 따라)
        if self.env_cfg["visualize_target"]:
            self.target = self.scene.add_entity(
                morph=gs.morphs.Mesh(
                    file="meshes/sphere.obj",  # 목표 지점 메시 파일
                    scale=0.05,  # 크기
                    fixed=True,  # 고정된 물체
                    collision=False,  # 충돌 감지 비활성화
                ),
                surface=gs.surfaces.Rough(
                    diffuse_texture=gs.textures.ColorTexture(
                        color=(1.0, 0.5, 0.5),  # 목표 지점 색상 (붉은 계열)
                    ),
                ),
            )
        else:
            self.target = None  # 목표 지점 비활성화

        # 카메라 추가 (시각화 여부에 따라)
        if self.env_cfg["visualize_camera"]:
            self.cam = self.scene.add_camera(
                res=(640, 480),  # 해상도 설정
                pos=(3.5, 0.0, 2.5),  # 카메라 위치
                lookat=(0, 0, 0.5),  # 카메라 시점
                fov=30,  # 시야각
                GUI=True,  # GUI 사용 여부
            )

        # 드론 추가
        self.base_init_pos = torch.tensor(self.env_cfg["base_init_pos"], device=self.device)  # 초기 위치
        self.base_init_quat = torch.tensor(self.env_cfg["base_init_quat"], device=self.device)  # 초기 쿼터니언
        self.inv_base_init_quat = inv_quat(self.base_init_quat)  # 초기 쿼터니언의 역변환
        self.drone = self.scene.add_entity(gs.morphs.Drone(file="urdf/drones/cf2x.urdf"))  # 드론 로드

        # 시뮬레이션 환경 빌드
        self.scene.build(n_envs=num_envs)

        # 보상 함수 준비 및 스케일 적용
        self.reward_functions, self.episode_sums = dict(), dict()
        for name in self.reward_scales.keys():
            self.reward_scales[name] *= self.dt  # 보상 값에 시뮬레이션 시간 스텝 적용
            self.reward_functions[name] = getattr(self, "_reward_" + name)  # 보상 함수 참조 저장
            self.episode_sums[name] = torch.zeros((self.num_envs,), device=self.device, dtype=gs.tc_float)

        # 버퍼 초기화
        self.obs_buf = torch.zeros((self.num_envs, self.num_obs), device=self.device, dtype=gs.tc_float)  # 관측 버퍼
        self.rew_buf = torch.zeros((self.num_envs,), device=self.device, dtype=gs.tc_float)  # 보상 버퍼
        self.reset_buf = torch.ones((self.num_envs,), device=self.device, dtype=gs.tc_int)  # 리셋 버퍼
        self.episode_length_buf = torch.zeros((self.num_envs,), device=self.device, dtype=gs.tc_int)  # 에피소드 길이 버퍼
        self.commands = torch.zeros((self.num_envs, self.num_commands), device=self.device, dtype=gs.tc_float)  # 명령 버퍼

        # 행동 및 이전 행동 버퍼
        self.actions = torch.zeros((self.num_envs, self.num_actions), device=self.device, dtype=gs.tc_float)
        self.last_actions = torch.zeros_like(self.actions)

        # 드론의 상태 변수 버퍼
        self.base_pos = torch.zeros((self.num_envs, 3), device=self.device, dtype=gs.tc_float)  # 위치
        self.base_quat = torch.zeros((self.num_envs, 4), device=self.device, dtype=gs.tc_float)  # 방향 (쿼터니언)
        self.base_lin_vel = torch.zeros((self.num_envs, 3), device=self.device, dtype=gs.tc_float)  # 선속도
        self.base_ang_vel = torch.zeros((self.num_envs, 3), device=self.device, dtype=gs.tc_float)  # 각속도
        self.last_base_pos = torch.zeros_like(self.base_pos)  # 이전 위치

        self.extras = dict()  # 추가적인 로그 정보 저장을 위한 딕셔너리

    # 이후 코드에서는 step(), reset(), 보상 함수 등의 세부 기능이 구현됨

```